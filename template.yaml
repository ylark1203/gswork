theme: default # default || classic || dark
organization: State Key Lab of CAD&CG, Zhejiang University
twitter: '@twitter'
title: 'RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars'
journal: "CVPR'25"
resources:
  paper: https://arxiv.org
  arxiv: https://arxiv.org
  code: https://github.com/denkiwakame/academic-project-template
  # video: https://www.youtube.com/embed/onbnb_D1wC8?si=xJczUv716Lt5aO4l&amp;start=1150
  # demo: https://colab.research.google.com/
  # huggingface: https://huggingface.co/
description: academic projectpage template that supports markdown and KaTeX

image: https://denkiwakame.github.io/academic-project-template/teaser.jpg
url: https://denkiwakame.github.io/academic-project-template
speakerdeck: # speakerdeck slide ID
authors:
  - name: Linzhou Li
    affiliation: []
    url: ""
  - name: Yumeng Li
    affiliation: []
    url: ""
  - name: Yanlin Weng
    affiliation: []
    url: https://person.zju.edu.cn/en/ylweng
  - name: Youyi Zheng
    affiliation: []
    url: https://www.zyouyi.com/
  - name: Kun Zhou
    affiliation: []
    url: http://kunzhou.net/
affiliations:
  - State Key Lab of CAD&CG, Zhejiang University
# meta:
#   - '*work done while she was interning at Pixel Genius Lab.'
bibtex: >
  @inproceedings{li2025rgbavatar,
    title={RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars},
    author={Li, Linzhou and Li, Yumeng and Weng, Yanlin and Zheng, Youyi and Zhou, Kun},
    booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    year={2025},
  }

teaser: teaser.jpg
abstract: |
  We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings.

body:
  - title: Live Demo
    text: |
      </div>
        <video src="live_demo_s.mp4" controls uk-video="autoplay: False"></video>
      </div>

  - title: Abstract
    text: |
      We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings.

  - title: Approach
    text: |
      ### Reduced Gaussian Blendshape Representation
      
      </div>
        <div>
          <img src="pipeline.jpg" alt="">
        </div>
      </div>
      
      RGBAvatar represents the head avatar with a base model $G_0$ and a reduced set of Gaussian blendshapes $\{\Delta G_i\}_{i=1}^{20}$, each parametized as Gaussian attributes. For an input video frame $I_t$, we first track the FLAME parameters $\theta$ and generate FLAME mesh $M^{\theta}$. Then, an MLP $\mathcal{F}$ is used to map the FLAME parameters $\theta$ to the reduced blendshape weights $\psi$. The Gaussian model $G^{\psi}$ of the animated avatar is generated through linear blending with $\psi$. Finally, Gaussians are transformed into the deformed space for rendering according to the deformation of mesh triangles.

      ### Batch-parallel Gaussian Rasterization

      </div>
        <div class="uk-flex uk-flex-center">
          <img src="batch.png" alt="" class="uk-width-2-3@m uk-width-1-2@l">
        </div>
      </div>

      To accelerate the training process, we propose a batch-parallel Gaussian rasterization method. We split Gaussian Splatting into two stages and perform GPU-CPU synchronization only once per training step. We achieve 100\% Stream Processor utilization and 630 images per second training throughput on a single RTX 3090 GPU.

  - title: Results
    text: |
      ### Blendshape Visualization

      </div>
        <div class="uk-flex uk-flex-center">
          <img src="bs_vis.jpg" alt="" class="uk-width-3-4@l">
        </div>
      </div>

      We show 10 out of 20 blendshapes learned by our method for two subjects. The blendshapes are diverse and adapted to each subject.

      ### Performance Comparisons

      | **Method** | **Training Time** ⬇️ | **Rendering FPS** ⬆️| **PSNR ⬆️** | **SSIM ⬆️** | **LPIPS ⬇️** |
      |---------------------|------------|------------|------------|------------|------------|
      | SplattingAvatar     | 21min      | 300        | 29.33      | 0.9348     | 0.1033     |
      | GaussianAvatars     | 14min      | 177        | 29.48      | 0.9474     | 0.0819     |
      | FlashAvatar         | 17min      | 304        | 28.04      | 0.9168     | 0.1045     |
      | MonoGaussianAvatar  | 9h         | 17         | 29.27      | 0.9475     | **0.0704** |
      | GaussianBlendShapes | 20min      | 267        | 30.35      | 0.9497     | 0.0876     |
      | **RGBAvatar**       | **81s**    | **398**    | **31.19**  | **0.9564** | 0.0737     |

      We report the training time and run time FPS on RTX 3090. Note that we train GaussianAvatars for 40k iterations for monocular setting. And the run time calculation includes both rendering and animation. We report *PSNR/SSIM/LPIPS* on test set of INSTA dataset.

  - title: Video
    text: |
      </div>
        <video src="video_public_s.mp4" controls uk-video="autoplay: False"></video>
      </div>

projects: # relevant projects
  - title: Relevant Project I
    description: abstract text
    img: https://getuikit.com/docs/images/light.jpg
    journal: "ABCD'23"
    url: https://denkiwakame.github.io/academic-project-template/
  # - title: Relevant Project II
  #   description: abstract text
  #   img: 001.jpg
  #   journal: "EFGH'22"
  #   url: https://denkiwakame.github.io/academic-project-template/
  # - title: Relevant Project III
  #   description: abstract text
  #   img: 002.jpg
  #   journal: "IJKL'22"
  #   url: https://denkiwakame.github.io/academic-project-template/
